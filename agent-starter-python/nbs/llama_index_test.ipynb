{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6419b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install llama-index-llms-google-genai llama-index llama-index-llms-ollama -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b283e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1488a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from google.genai import types\n",
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"gemma3:4b\", request_timeout=60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7a90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "\n",
    "agent = FunctionAgent(\n",
    "    tools=[multiply, add],\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d47ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import ToolCallResult\n",
    "\n",
    "\n",
    "async def run_agent_verbose(query: str):\n",
    "    handler = agent.run(query)\n",
    "    async for event in handler.stream_events():\n",
    "        if isinstance(event, ToolCallResult):\n",
    "            print(\n",
    "                f\"Called tool {event.tool_name} with args {event.tool_kwargs}\\nGot result: {event.tool_output}\"\n",
    "            )\n",
    "\n",
    "    return await handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decd2836",
   "metadata": {},
   "outputs": [
    {
     "ename": "WorkflowRuntimeError",
     "evalue": "Error in step 'run_agent_step': registry.ollama.ai/library/gemma3:4b does not support tools (status code: 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/workflows/context/context.py:696\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager)\u001b[39m\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     new_ev = \u001b[38;5;28;01mawait\u001b[39;00m instrumented_step(**kwargs)\n\u001b[32m    697\u001b[39m     kwargs.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/llama_index_instrumentation/dispatcher.py:368\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m368\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/base_agent.py:391\u001b[39m, in \u001b[36mBaseWorkflowAgent.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    389\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.take_step(\n\u001b[32m    392\u001b[39m     ctx,\n\u001b[32m    393\u001b[39m     ev.input,\n\u001b[32m    394\u001b[39m     tools,\n\u001b[32m    395\u001b[39m     memory,\n\u001b[32m    396\u001b[39m )\n\u001b[32m    398\u001b[39m ctx.write_event_to_stream(agent_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/function_agent.py:120\u001b[39m, in \u001b[36mFunctionAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.streaming:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     last_chat_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_streaming_response(\n\u001b[32m    121\u001b[39m         ctx, current_llm_input, tools\n\u001b[32m    122\u001b[39m     )\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/llama_index/core/agent/workflow/function_agent.py:75\u001b[39m, in \u001b[36mFunctionAgent._get_streaming_response\u001b[39m\u001b[34m(self, ctx, current_llm_input, tools)\u001b[39m\n\u001b[32m     74\u001b[39m last_chat_response = ChatResponse(message=ChatMessage())\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m last_chat_response \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m     76\u001b[39m     tool_calls = \u001b[38;5;28mself\u001b[39m.llm.get_tool_calls_from_response(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     77\u001b[39m         last_chat_response, error_on_no_tool_call=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     78\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py:89\u001b[39m, in \u001b[36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_async_llm_chat.<locals>.wrapped_gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m f_return_val:\n\u001b[32m     90\u001b[39m         dispatcher.event(\n\u001b[32m     91\u001b[39m             LLMChatInProgressEvent(\n\u001b[32m     92\u001b[39m                 messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m             )\n\u001b[32m     96\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py:470\u001b[39m, in \u001b[36mOllama.astream_chat.<locals>.gen\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    468\u001b[39m all_tool_calls = []\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m r[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/ollama/_client.py:682\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n",
      "\u001b[31mResponseError\u001b[39m: registry.ollama.ai/library/gemma3:4b does not support tools (status code: 400)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m run_agent_verbose(\u001b[33m\"\u001b[39m\u001b[33mWhat is (121 + 2) * 5?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(response))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mrun_agent_verbose\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, ToolCallResult):\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m      9\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalled tool \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent.tool_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with args \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent.tool_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGot result: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent.tool_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m handler\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/workflows/workflow.py:479\u001b[39m, in \u001b[36mWorkflow.run.<locals>._run_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_raised:\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    477\u001b[39m     ctx.write_event_to_stream(StopEvent())\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_raised\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m we_done:\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# cancel the stream\u001b[39;00m\n\u001b[32m    483\u001b[39m     ctx.write_event_to_stream(StopEvent())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/development/livekit_llm_pipeline/agent-starter-python/.venv/lib/python3.12/site-packages/workflows/context/context.py:705\u001b[39m, in \u001b[36mContext._step_worker\u001b[39m\u001b[34m(self, name, step, config, stepwise, verbose, checkpoint_callback, run_id, service_manager, resource_manager)\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.retry_policy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m WorkflowRuntimeError(\n\u001b[32m    706\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in step \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!s}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    707\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    709\u001b[39m     delay = config.retry_policy.next(\n\u001b[32m    710\u001b[39m         retry_start_at + time.time(), attempts, e\n\u001b[32m    711\u001b[39m     )\n\u001b[32m    712\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    713\u001b[39m         \u001b[38;5;66;03m# We're done retrying\u001b[39;00m\n",
      "\u001b[31mWorkflowRuntimeError\u001b[39m: Error in step 'run_agent_step': registry.ollama.ai/library/gemma3:4b does not support tools (status code: 400)"
     ]
    }
   ],
   "source": [
    "response = await run_agent_verbose(\"What is (121 + 2) * 5?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c99e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent-starter-python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
